{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron node\n",
    "<table>\n",
    "<tr>\n",
    "<td> <img src=\"img/perceptron_node.png\" alt=\"Drawing\" style=\"width: 570px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The above [image](https://skymind.ai/wiki/neural-network) shows a one perceptron node, where weights are first multiplied by the input vlaues, and are summed. The sum is then passed through an activation function, and the output is recorded.\n",
    "\n",
    "Lets take a simple example of only two 2-input neurons, $x$, weights $w$ and some bias $b$, that uses the sigmoid activation function, $f(x)$ for the activation:\n",
    "\n",
    "\\begin{equation}\n",
    "x = \\left(\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right); \n",
    "w = \\left({\\begin{array}{cc} 0 & 1 \\end{array}}\\right); \n",
    "b = 4 \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w.x + b  = \n",
    "\\left({\\begin{array}{cc} 0 & 1 \\end{array}}\\right)\n",
    "%\\left(\\begin{array}{cc} 1 & 0\\\\ 0 & 1 \\end{array}\\right)\n",
    "\\left(\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right) + 4 = 7\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = f(7) = \\frac{1}{1+e^{-7}} = 0.99\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 0.999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def netInput(x, w, b):\n",
    "    return np.dot(w, x) + b \n",
    "\n",
    "x = np.array([2, 3])\n",
    "w = np.array([0, 1]) \n",
    "b = 4 \n",
    "\n",
    "n = netInput(x, w, b) \n",
    "f = sigmoid(n)\n",
    "print ('output', round(f,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forwad Neural Network\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> <img src=\"img/feedforward.png\" alt=\"Drawing\" style=\"width: 570px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Let $h_1$, $h_2$, $o_1$ be the outputs of the neurons as shown in above [image](https://miro.medium.com/max/550/1*x6KWjKTOBhUYL0MRX4M3oQ.png).\n",
    "\n",
    "Continuing the same example of two 2-input neurons, $x$, weights $w$ and bias $b=4$, \n",
    "\n",
    "\\begin{equation}\n",
    "h_1 = h_2 = f(w.x + b)  = f(w.[2,3]+4) = f(7) = 0.999\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "o_1  = f(w.[h_1, h_2] + b) = f((0*h_1)+(1*h_2) + b) = f(0.999) = 0.993\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1: 0.993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def netInput(self, x):\n",
    "        y = np.dot(self.w, x) + self.b\n",
    "        return sigmoid(y)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    '''\n",
    "    A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "    Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 4\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        w = np.array([0, 1])\n",
    "        b = 4\n",
    "    \n",
    "        # The Neuron class here is from the previous section\n",
    "        self.h1 = Neuron(w, b)\n",
    "        self.h2 = Neuron(w, b)\n",
    "        self.o1 = Neuron(w, b)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.netInput(x)\n",
    "        out_h2 = self.h2.netInput(x)\n",
    "\n",
    "        # The inputs for o1 are the outputs from h1 and h2\n",
    "        out_o1 = self.o1.netInput(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "\n",
    "network = NeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print ('o1:', round(network.feedforward(x),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss:\n",
    "\n",
    "Suppose the output $o_1$ classifies a gender, such as 0 indicate males and 1 indicate 0. The input data that features weight and height after some pre-processing looks like the following. Here $y_{true}$ is the true gender and $y_{pred}$ is the predicted outcome, say from some model. Original tutorial can be found [here](https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9).\n",
    "\n",
    "| Name     |Weight(-135 lb)| Height(-66 in)  | $y_{true}$(Gender) | $y_{pred}$(Gender) | \n",
    "|  ------  |  ------      | ------           |  ------            |   ------           |        \n",
    "| Margaret |    -2        |   -1             |     1              |   0                |\n",
    "| Chris    |    25        |    6             |     0              |   0                |\n",
    "| John     |    17        |    4             |     0              |   0                |\n",
    "| Sadia    |     5        |   -3             |     1              |   0                |\n",
    "\n",
    "Loss for this network is \n",
    "\n",
    "\\begin{equation*} \n",
    "\\hat{\\sigma}^2 = \\frac{\\Sigma_i^n (Y_i - \\hat{Y_i})^2}{n} = \\frac{(1 + 0 + 0 + 1)}{4} = 0.5\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true, y_hat):\n",
    "    res = y_true - y_hat\n",
    "    cost = np.average(res**2, axis=0)\n",
    "    return (cost)\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Back Propagation\n",
    "\n",
    "Lets minimize the loss, $L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)$ of this neural network. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> <img src=\"img/feedforward2.png\" alt=\"Drawing\" style=\"width: 570px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "For simplicity, lets consider only one input as Margaret, bias, b=0, and weights, w_1, w_2, w_3, w_4, w_5, w_6=1. Loss function in this case is\n",
    "\n",
    "\\begin{equation*} \n",
    "L = \\hat{\\sigma}^2 = \\frac{\\Sigma_i^n (Y_i - \\hat{Y_i})^2}{n} = (1 - \\hat{Y_i})^2\n",
    "\\end{equation*}\n",
    "\n",
    "Lets evaluate the change in loss w.r.t weight $w1$ for input Margaret. Chain rule allows us to write the following, which is also called back propagation.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\hat{Y_1}} \\frac{\\partial \\hat{Y_1}} {\\partial h_1} \\frac{\\partial h_1} {\\partial w_1}\n",
    "\\end{equation*}\n",
    "\n",
    "Lets first write another important equation, the derivative of sigmoid function, that we will need for the numerical calculations.\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = \\frac{1}{1-e^{-x}} ; \\\\\n",
    "f^{\\prime}(x) = \\frac{e^{-x}}{(1-e^{-x})^2} = f(x) (1 - f(x))\n",
    "\\end{equation*}\n",
    "\n",
    "For input Margaret,$h_1$, $h_2$ and $\\hat{Y_1}$ = $o_1$ are: \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "h_1 &=& f(w_1 x_1 + w_2 x_2 + b_1) = f(-2-1+0) = 0.0474\\\\\n",
    "h_2 &=& f(w_3 x_1 + w_6 x_2 + b_3) = 0.0474 \\\\\n",
    "\\hat{Y_1} &=& f(w_5 h_1 + w_6 h_2 + b_3) = f(0.0474 + 0.0474 + 0.0) = 0.524\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Lets find all other partial derivatives.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial \\hat{Y_1}} = -2 (1 - \\hat{Y_1}) = -2(1 - 0.524) = -0.952\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial \\hat{Y_1}}{\\partial h_1} = x_5 * f^{\\prime}(w_5 h_1 + w_6 h_2 + b_3 ) = 1 * f^{\\prime}(0.0474+0.0474+0) = 1 * f(0.0948)(1-f(0.0948)) = 0.249\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial h_1}{\\partial w_1} = x_1 * f^{\\prime}(w_1 x_1 + w_2 x_2 + b_1 ) = -2 * f^{\\prime}(-2-1+0) = -2 *  f(-3)(1-f(-3)) = -0.0904 \n",
    "\\end{equation*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial w_1} = -0.952 * 0.249 *  -0.0904  = 0.0124\n",
    "\\end{equation*}\n",
    "\n",
    "This tells us that if we increase $w_1$, then $L$ will increase only a tiny bit.\n",
    "\n",
    "## Training: Stochastic Gradient Descent\n",
    "\n",
    "We learned about all the tools needed to train a neural network. The training is generally performed by optimizing an algorithm called stochastic gradient descent (SGD) that tells us how to change our weights and biases to minimize loss.  \n",
    "\n",
    "The parameters are constantly updated, such as\n",
    "\n",
    "\\begin{equation*}\n",
    "w_1 \\leftarrow w_1 - \\alpha \\frac{\\partial L}{\\partial w_1}\n",
    "\\end{equation*}\n",
    "\n",
    "Here $\\alpha$ is called the learning rate. If $\\frac{\\partial L}{\\partial w_1}$ is positive, $w_1$ will decrease, which will make $L$ decrease, and vice-versa. A detailed [tutorial](gradientDescent.ipynb) on this topic can be found here. Lets take a moment to go through it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Complete Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.596\n",
      "Epoch 50 loss: 0.134\n",
      "Epoch 100 loss: 0.049\n",
      "Epoch 150 loss: 0.027\n",
      "Epoch 200 loss: 0.018\n",
      "Epoch 250 loss: 0.013\n",
      "Epoch 300 loss: 0.010\n",
      "Epoch 350 loss: 0.008\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 500 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_hat):\n",
    "    res = y_true - y_hat\n",
    "    cost = np.average(res**2, axis=0)\n",
    "    return cost\n",
    "\n",
    "def plotLoss(losses, epochs):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    ax.scatter(epochs, losses, color='blue',alpha=0.5, s=40, lw=0)\n",
    "    plt.title('Training')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    fig.patch.set_facecolor('white') #sets the color of the border\n",
    "    plt.show()\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    '''\n",
    "      A neural network with:\n",
    "        - 2 inputs\n",
    "        - a hidden layer with 2 neurons (h1, h2)\n",
    "        - an output layer with 1 neuron (o1)\n",
    "      '''\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements.\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "        - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "        - all_y_trues is a numpy array with n elements.\n",
    "          Elements in all_y_trues correspond to those in data.\n",
    "        '''\n",
    "        alpha = 0.1\n",
    "        epochs = 1000 # number of times to loop through the entire dataset\n",
    "        iterations = []\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_hat = o1\n",
    "\n",
    "                # --- Calculate partial derivatives.\n",
    "                # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_yhat = -2 * (y_true - y_hat)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_yhat_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_yhat_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_yhat_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_yhat_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_yhat_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= alpha * d_L_d_yhat * d_yhat_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= alpha * d_L_d_yhat * d_yhat_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= alpha * d_L_d_yhat * d_yhat_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= alpha * d_L_d_yhat * d_yhat_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= alpha * d_L_d_yhat * d_yhat_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= alpha * d_L_d_yhat * d_yhat_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= alpha * d_L_d_yhat * d_yhat_d_w5\n",
    "                self.w6 -= alpha * d_L_d_yhat * d_yhat_d_w6\n",
    "                self.b3 -= alpha * d_L_d_yhat * d_yhat_d_b3\n",
    "\n",
    "            # --- Calculate total loss at the end of each epoch\n",
    "            y_hats = np.apply_along_axis(self.feedforward, 1, data)\n",
    "            #print(\"y_hats:\", y_hats)\n",
    "            loss = mse_loss(all_y_trues, y_hats)\n",
    "            losses.append(loss)\n",
    "            iterations.append(epoch)\n",
    "            \n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))               \n",
    "                \n",
    "        return losses, iterations \n",
    "        \n",
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Margaret\n",
    "  [25, 6],   # Chris\n",
    "  [17, 4],   # John\n",
    "  [-15, -6], # Sadia\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Margaret\n",
    "  0, # Chris\n",
    "  0, # John\n",
    "  1, # Sadia\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = NeuralNetwork()\n",
    "losses, epochs = network.train(data, all_y_trues)\n",
    "\n",
    "plotLoss(losses, epochs)\n",
    "#print (losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Lets make quick predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.970\n",
      "Frank: 0.040\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "Emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "Frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(Emily)) \n",
    "print(\"Frank: %.3f\" % network.feedforward(Frank)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
